{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0391298c",
   "metadata": {},
   "source": [
    "# Reinforcement Learning - RIDER Project\n",
    "\n",
    "<img src=\"logo.jpeg\" style=\"float: left; width: 15%\" />\n",
    "\n",
    "\n",
    "2022-2023 Marc-Antoine Oudotte, Cl√©ment Garancini, Victor Barberteguy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a68bbf",
   "metadata": {},
   "source": [
    "# Deep Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e43aca",
   "metadata": {},
   "source": [
    "This notebook presents the algorithm, settings and results of the Deep Q-Learning (DQN) algorithm on the three levels of the Rider game"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bcc103",
   "metadata": {},
   "source": [
    "## 1 - Environment and algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f834987",
   "metadata": {},
   "source": [
    "We start by installing the necessary libraries and loading the Unity Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd28776",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't have the libraries, run this cell\n",
    "### THIS REQUIRES PYTHON 3.7 OR HIGHER AND IS LIKELY TO DOWNGRADE SOME OF YOUR LIBRARIES\n",
    "### IF YOU JUST WANT TO SEE THE RESULT, GO TO PART 3, ELSE YOU CAN RUN ON A VIRTUAL ENV\n",
    "### You can also use the file RiderDQN.py that is best fitted to run the algorithm.\n",
    "\n",
    "!pip install gymnasium imageio ipython ipywidgets nnfigs numpy pandas pygame seaborn torch tqdm matplotlib mlagents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58368061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from mlagents_envs.envs.unity_gym_env import UnityToGymWrapper\n",
    "from mlagents_envs.environment import UnityEnvironment\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "import copy\n",
    "from tqdm.notebook import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c441fd56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input the number of the level that will be played by the algorithm\n",
    "\n",
    "LEVEL = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757bf83e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the environment\n",
    "\n",
    "# Input True if you want the game to render graphically\n",
    "Graphics = False\n",
    "\n",
    "#Input the path to the Build\n",
    "PATH = \"/Users/victorbarberteguy/Desktop/3A/INF581/RL/Rider/Build_Level\"+str(LEVEL)+\".app\"\n",
    "\n",
    "uEnv = UnityEnvironment(PATH, worker_id=0, seed=1, no_graphics=not(Graphics))\n",
    "env = UnityToGymWrapper(uEnv, uint8_visual = False, flatten_branched= True,  allow_multiple_obs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0402ea",
   "metadata": {},
   "source": [
    "We now define the constants of the DQN, and save them at runtime in a .txt file (in the active directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53558739",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "EPISODES = 200\n",
    "LR = 0.001\n",
    "MEM_SIZE = 10000\n",
    "BATCH_SIZE = 72\n",
    "GAMMA = 0.95\n",
    "EXPLORATION_MAX = 1.0\n",
    "EXPLORATION_DECAY = 0.999\n",
    "EXPLORATION_MIN = 0.001\n",
    "sync_freq = 5\n",
    "\n",
    "file = open(\"settingsDQN.txt\", 'w')\n",
    "settings = ['\\nEPISODES  = ' + str(EPISODES),'\\nLR  = ' + str(LR), '\\nMEM_SIZE  = ' + str(MEM_SIZE),'\\nBATCH_SIZE  = ' + str(BATCH_SIZE),'\\nGAMMA  = ' + str(GAMMA),'\\nEXPLORATION_MAX  = ' + str(EXPLORATION_MAX),'\\nEXPLORATION_DECAY  = ' + str(EXPLORATION_DECAY),'\\nEXPLORATION_MIN  = ' + str(EXPLORATION_MIN),'\\nsync_freq = ' + str(sync_freq) ]\n",
    "file.writelines(settings)\n",
    "file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ed0d497",
   "metadata": {},
   "source": [
    "We create a network class that will be a parameter of our DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17347a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.input_shape = 5 # our observation_space is 5\n",
    "        self.action_space = 2 # our action space is 2\n",
    "\n",
    "        # We chose a shallow network with two layers and ReLu activation functions\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.input_shape, 1024)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.fc3 = nn.Linear(512, self.action_space)\n",
    "\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=LR)\n",
    "        self.loss = nn.MSELoss()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4520e4f",
   "metadata": {},
   "source": [
    "Another parameter of our DQN will be a replay buffer. This feature stabilizes the learning by replaying in a random order the `BATCH SIZE` previous episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0312540f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    def __init__(self):\n",
    "        self.memory = deque(maxlen=MEM_SIZE)\n",
    "    \n",
    "    def add(self, experience):\n",
    "        self.memory.append(experience)\n",
    "    \n",
    "    def sample(self):\n",
    "        minibatch = random.sample(self.memory, BATCH_SIZE)\n",
    "\n",
    "        state1_batch = torch.stack([s1 for (s1,a,r,s2,d) in minibatch])\n",
    "        action_batch = torch.tensor([a for (s1,a,r,s2,d) in minibatch])\n",
    "        reward_batch = torch.tensor([r for (s1,a,r,s2,d) in minibatch])\n",
    "        state2_batch = torch.stack([s2 for (s1,a,r,s2,d) in minibatch])\n",
    "        done_batch = torch.tensor([d for (s1,a,r,s2,d) in minibatch])\n",
    "\n",
    "        return (state1_batch, action_batch, reward_batch, state2_batch, done_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9060c91d",
   "metadata": {},
   "source": [
    "We can now define our main class DQN with a secpnd feature : the target network TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd738089",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN:\n",
    "    def __init__(self):\n",
    "        self.replay = ReplayBuffer() \n",
    "        self.exploration_rate = EXPLORATION_MAX\n",
    "        self.network = Network()\n",
    "        \n",
    "        # Target network\n",
    "        self.network2 = copy.deepcopy(self.network) \n",
    "        self.network2.load_state_dict(self.network.state_dict())\n",
    "\n",
    "\n",
    "    def choose_action(self, observation):\n",
    "        if random.random() < self.exploration_rate:\n",
    "            return env.action_space.sample()\n",
    "\n",
    "        # Convert observation to PyTorch Tensor\n",
    "        state = torch.tensor(observation).float().detach()\n",
    "        #state = state.to(DEVICE)\n",
    "        state = state.unsqueeze(0)\n",
    "\n",
    "        # Get Q(s,.)\n",
    "        q_values = self.network(state)\n",
    "\n",
    "        # Choose the action to play\n",
    "        action = torch.argmax(q_values).item()\n",
    "\n",
    "        return action\n",
    "    \n",
    "    def learn(self):\n",
    "        if len(self.replay.memory)< BATCH_SIZE:\n",
    "            return\n",
    "\n",
    "        # Sample minibatch s1, a1, r1, s1', done_1, ... , sn, an, rn, sn', done_n\n",
    "        state1_batch, action_batch, reward_batch, state2_batch, done_batch = self.replay.sample()\n",
    "\n",
    "        # Compute Q values\n",
    "        q_values = self.network(state1_batch).squeeze()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Compute next Q values\n",
    "            next_q_values = self.network2(state2_batch).squeeze()\n",
    "\n",
    "        batch_indices = np.arange(BATCH_SIZE, dtype=np.int64)\n",
    "\n",
    "        predicted_value_of_now = q_values[batch_indices, action_batch]\n",
    "        predicted_value_of_future = torch.max(next_q_values, dim=1)[0]\n",
    "\n",
    "        # Compute the q_target\n",
    "        q_target = reward_batch + GAMMA * predicted_value_of_future * (1-(done_batch).long())\n",
    "\n",
    "        # Compute the loss (c.f. self.network.loss())\n",
    "        loss = self.network.loss(q_target, predicted_value_of_now)\n",
    "\n",
    "        # Complute ùõÅQ\n",
    "        self.network.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.network.optimizer.step()\n",
    "\n",
    "        self.exploration_rate *= EXPLORATION_DECAY\n",
    "        self.exploration_rate = max(EXPLORATION_MIN, self.exploration_rate)\n",
    "        \n",
    "    def returning_epsilon(self):\n",
    "        return self.exploration_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21afebd9",
   "metadata": {},
   "source": [
    "## 2 - Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51c95df",
   "metadata": {},
   "source": [
    "We start the training of our DQN that will last `EPISODES` episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1cc450",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN()\n",
    "\n",
    "best_reward = 0\n",
    "average_reward = 0\n",
    "episode_number = []\n",
    "average_reward_number = []\n",
    "\n",
    "success = False # Success on one try\n",
    "\n",
    "n_success_10_try = 0 #number of successes on 10 consecutive tries\n",
    "\n",
    "average_success_10_try = [] #store the number of successes on 10 tries to plot it\n",
    "\n",
    "j=0\n",
    "dim = 5 # dimesion of observation space\n",
    "print(\"BEGINNING TRAINING\")\n",
    "\n",
    "for i in tqdm(range(1, EPISODES+1)):\n",
    "\n",
    "    print(\"episode \" + str(i) + \" playing...\")\n",
    "    \n",
    "    state = env.reset()\n",
    "    state = np.reshape(state, [1, dim])\n",
    "    score = 0\n",
    "\n",
    "    while True:\n",
    "        j+=1\n",
    "\n",
    "        action = agent.choose_action(state)\n",
    "\n",
    "        state_, reward, done, info = env.step(action)\n",
    "        state_ = np.reshape(state_, [1, dim])\n",
    "        state = torch.tensor(state).float()\n",
    "        state_ = torch.tensor(state_).float()\n",
    "\n",
    "        exp = (state, action, reward, state_, done)\n",
    "        agent.replay.add(exp)\n",
    "        \n",
    "        # Will effectively learn only if the replay buffer is full\n",
    "        agent.learn()\n",
    "\n",
    "        state = state_\n",
    "        score += reward\n",
    "\n",
    "        # As in the script agent.cs (in Unity), we set the reward to 200 if the goal is reached,\n",
    "        #   we test the success with this condition\n",
    "        \n",
    "        if(reward == 200) :\n",
    "            success = True\n",
    "            \n",
    "        # Synchronization ofthe target network\n",
    "        if j % sync_freq == 0:\n",
    "            agent.network2.load_state_dict(agent.network.state_dict())\n",
    "\n",
    "        # Episode is finished\n",
    "        if done:\n",
    "            if score > best_reward:\n",
    "                best_reward = score\n",
    "\n",
    "            average_reward += score \n",
    "            \n",
    "            if(success): #goal reached\n",
    "                success = False\n",
    "                n_success_10_try += 1\n",
    "\n",
    "\n",
    "            if i%10==0:\n",
    "                print(\"Episode {} Average Reward {} Best Reward {} Last Reward {} Epsilon {} Success = {}/10\".format(i, average_reward/i, best_reward, score, agent.returning_epsilon(), n_success_10_try))\n",
    "                average_success_10_try.append(n_success_10_try)\n",
    "                n_success_10_try = 0\n",
    "\n",
    "            break\n",
    "            \n",
    "        \n",
    "        episode_number.append(i)\n",
    "        average_reward_number.append(average_reward/i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e0c7dd9",
   "metadata": {},
   "source": [
    "## 3 - Save the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4964ae",
   "metadata": {},
   "source": [
    "We plot and save in the active directory the reward vs the number of episodes, and the number of successes every 10 tries.\n",
    "On the second plot, we traced y=7 as 68% of success is often used to know if the training is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c80547",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplot(1,2,1)\n",
    "plt.plot(episode_number, average_reward_number)\n",
    "plt.title(\"Plot of the reward\")\n",
    "plt.xlabel('Episodes ')\n",
    "plt.ylabel('Reward ')\n",
    "\n",
    "plt.subplot(1, 2, 2) # index 2\n",
    "x2 = [10*(i+1) for i in range(int(EPISODES/10))]\n",
    "plt.scatter(x2, average_success_10_try)\n",
    "plt.plot(x2, [7 for i in range(len(x2))], color = 'r')\n",
    "plt.ylim(-1,11)\n",
    "plt.title(\"Successes each 10 tries\")\n",
    "plt.xlabel('Episodes')\n",
    "plt.ylabel('N success every 10 tries')\n",
    "\n",
    "plt.savefig(\"ResultsDQN\")\n",
    "env.close()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "953a9af0",
   "metadata": {},
   "source": [
    "## 4 - Exploit the results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34a8db2",
   "metadata": {},
   "source": [
    "We will display some of the results we had after training our agent on the three levels so that the lector can have some visuals without running the code (it can take up to 10/15min with 200 episodes...).\n",
    "However, the interpretation of these results will be given in the report (.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9adc0",
   "metadata": {},
   "source": [
    "### 4.1 - Level 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072422ce",
   "metadata": {},
   "source": [
    "This level is quite simple. The main difficulty is to jump over a hole.\n",
    "<img src=\"level1.png\" alt=\"Level 1\" style=\"float: right; width: 50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "385ec57b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The settings of the try displayed are as follows:\n",
    "\n",
    "EPISODES  = 200\n",
    "LR  = 0.001 #dynamially changed during training\n",
    "MEM_SIZE  = 10000\n",
    "BATCH_SIZE  = 72\n",
    "GAMMA  = 0.95\n",
    "EXPLORATION_MAX  = 1.0\n",
    "EXPLORATION_DECAY  = 0.999\n",
    "EXPLORATION_MIN  = 0.001\n",
    "sync_freq = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "31f2f5c2",
   "metadata": {},
   "source": [
    "Here are the results obtained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b9759fac",
   "metadata": {},
   "source": [
    "<img src=\"LV1OPTI-DYNAMIC.png\" title=\"Results lvl1\" style=\"display:block;float: center; width: 50%\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3460c77b",
   "metadata": {},
   "source": [
    "Here is the final episode (when the agent is trained)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1a12c0e",
   "metadata": {},
   "source": [
    "<img src=\"LVL1.gif\" style=\"float: center; width: 50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "999211a3",
   "metadata": {},
   "source": [
    "### 4.2 - Level 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0bb65265",
   "metadata": {},
   "source": [
    "The level 2 is trickier inasmuch as the agent can find the goal easily, but will face failures when trying to reach the goal faster. Indeed, it will collide with the upper slope. We wanted to study how the agent would adapt his speed and increase its reward.\n",
    "<img src=\"level2.png\" alt=\"Level 2 - Speed Control\" style=\"float: right; width: 50%\" />\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f1192d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The settings of the try displayed are as follows:\n",
    "\n",
    "EPISODES  = 200\n",
    "LR  = 0.001\n",
    "MEM_SIZE  = 10000\n",
    "BATCH_SIZE  = 72\n",
    "GAMMA  = 0.8\n",
    "EXPLORATION_MAX  = 1.0\n",
    "EXPLORATION_DECAY  = 0.999\n",
    "EXPLORATION_MIN  = 0.001\n",
    "sync_freq = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "e266d1a6",
   "metadata": {},
   "source": [
    "Here are the results we obtained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5b93b1",
   "metadata": {},
   "source": [
    "<img src=\"LVL2OPTI.png\" style=\"float: center; width: 50%\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bc9890e6",
   "metadata": {},
   "source": [
    "Here is the final episode (when the agent is trained)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c67e0aed",
   "metadata": {},
   "source": [
    "<img src=\"LVL21.gif\" style=\"float: center; width: 50%\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98c7257",
   "metadata": {},
   "source": [
    "### 4.3 - Level 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347e6561",
   "metadata": {},
   "source": [
    "The level 3 is the hardest we have designed and the goal is to control spin. The agent has to land on the platforms correctly or it will collide with the slopes. When landed, it the optimal policy is to keep accelerating.\n",
    "<img src=\"level3.png\" style=\"float: right; width: 50%\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d6c90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The settings of the try displayed are as follows:\n",
    "\n",
    "EPISODES  = 200\n",
    "LR  = 0.001\n",
    "MEM_SIZE  = 10000\n",
    "BATCH_SIZE  = 72\n",
    "GAMMA  = 0.97\n",
    "EXPLORATION_MAX  = 1.0\n",
    "EXPLORATION_DECAY  = 0.999\n",
    "EXPLORATION_MIN  = 0.001\n",
    "sync_freq = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66524cd",
   "metadata": {},
   "source": [
    "Here are the results we obtained."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a0fae07",
   "metadata": {},
   "source": [
    "<img src=\"LVL3OPTI.png\" style=\"float: center; width: 50%\" />"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f88ac47e",
   "metadata": {},
   "source": [
    "Here is the final episode (when the agent is trained)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b88aef5f",
   "metadata": {},
   "source": [
    "<img src=\"LVL3.gif\" style=\"float: center; width: 50%\" />"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
